{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# AIC-25 Colab: Textual KIS (Clean Flow)\n",
    "\n",
    "Choose one path and follow the numbered steps.\n",
    "\n",
    "- Path A — Quickstart (use provided features): simplest, fastest\n",
    "- Path B — Recompute (SigLIP2): higher quality, recomputes features and model\n",
    "\n",
    "Steps overview\n",
    "1) Clone repo\n",
    "2) Install deps\n",
    "3) Download dataset\n",
    "4A) Quickstart setup (Path A) — OR — 4B) Recompute SigLIP2 (Path B)\n",
    "5) Start backend\n",
    "6) Run a KIS query and export CSV\n",
    "7) Zip for Codabench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "choose_path"
   },
   "outputs": [],
   "source": [
    "# Choose your path: 'quickstart' or 'recompute'\n",
    "PATH_CHOICE = 'recompute'  # <-- set to 'quickstart' for the simplest path\n",
    "print('Path:', PATH_CHOICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# Step 1) Clone the repo into /content/aic-25\n",
    "import os, shutil, subprocess\n",
    "REPO_URL = 'https://github.com/dnlqvu/hcm-AI-challenge-2024-main.git'\n",
    "TARGET_DIR = '/content/aic-25'\n",
    "if os.path.exists(TARGET_DIR):\n",
    "    shutil.rmtree(TARGET_DIR)\n",
    "print('Cloning', REPO_URL, '->', TARGET_DIR)\n",
    "subprocess.run(['git', 'clone', REPO_URL, TARGET_DIR], check=True)\n",
    "print(os.listdir(TARGET_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cd_root"
   },
   "outputs": [],
   "source": [
    "%cd /content/aic-25\n",
    "!echo 'CWD:' && pwd && echo 'Top-level:' && ls -la | head -n 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Step 2) Install backend + extractor dependencies\n",
    "%cd /content/aic-25/aic-24-BE\n",
    "!python -m pip install --quiet --upgrade pip\n",
    "!pip install --quiet -r requirements.txt\n",
    "# Check if critical packages installed successfully\n",
    "import sys\n",
    "try:\n",
    "    import uvicorn\n",
    "    import fastapi\n",
    "    print(\"✓ Backend dependencies installed\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Backend dependency missing: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "%cd /content/aic-25\n",
    "# Extras for local extraction & utilities\n",
    "!pip install --quiet opencv-python tqdm pillow open_clip_torch\n",
    "# Verify extraction dependencies\n",
    "try:\n",
    "    import cv2\n",
    "    import open_clip\n",
    "    import torch\n",
    "    print(f\"✓ Extraction dependencies installed (torch device: {'cuda' if torch.cuda.is_available() else 'cpu'})\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Extraction dependency missing: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_csv"
   },
   "outputs": [],
   "source": [
    "# Step 3) Upload AIC_2025_dataset_download_link.csv (or set CSV_PATH)\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # choose AIC_2025_dataset_download_link.csv\n",
    "CSV_PATH = next(iter(uploaded))\n",
    "print('Using CSV:', CSV_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_dataset"
   },
   "outputs": [],
   "source": [
    "# Step 3) Download dataset assets to example_dataset/ and extract\n",
    "%cd /content/aic-25\n",
    "!python tools/aic_cli.py download-dataset --csv $CSV_PATH --outdir example_dataset --extract\n",
    "!ls -la example_dataset | head -n 50\n",
    "# Sanity checks\n",
    "!test -d example_dataset/map-keyframes || echo 'MISSING: example_dataset/map-keyframes'\n",
    "!test -d example_dataset/clip-features-32 || echo 'MISSING: example_dataset/clip-features-32'\n",
    "!test -d example_dataset/media-info || echo 'MISSING: example_dataset/media-info'\n",
    "!test -d example_dataset/keyframes || echo 'MISSING: example_dataset/keyframes'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1fb281",
   "metadata": {
    "id": "pathA_quickstart"
   },
   "outputs": [],
   "source": "%cd /content/aic-25\nif PATH_CHOICE != 'recompute':\n    print('Skipping Recompute (PATH_CHOICE!=recompute)')\n    raise SystemExit(0)\nVIDEOS_DIR = 'example_dataset/Videos_L21_a'\nMODEL = 'ViT-L-16-SigLIP-256'  # Changed from 384 to avoid OOM\nPRETRAINED = 'webli'  # Correct pretrained tag for SigLIP models\nCLIP_LEN = 1.5\nDECODE_FPS = 1.5  # Reduced from 2.0 to save memory\nTARGET_FPS = 0.8  # Reduced from 1.0 to save memory\n# 4B.1 Smart sampling → extract exact frames (original indices) - with intelligent adaptive sampling\n!python tools/aic_cli.py sample-smart --strategy clip-delta --videos-dir $VIDEOS_DIR \\\n    --frames-dir aic-24-BE/data/video_frames --decode-fps $DECODE_FPS --target-fps $TARGET_FPS \\\n    --model $MODEL --pretrained $PRETRAINED --adaptive\n\n# 4B.2 Encode sampled frames with SigLIP-256 and write shards\n%cd /content/aic-25/aic-24-BE\nimport os, numpy as np, torch, pickle, gc\nfrom pathlib import Path\nfrom PIL import Image\nimport open_clip\nfrom tqdm import tqdm\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Loading {MODEL} with {PRETRAINED} on {device}...\")\n\n# Clear any existing models from memory\nif 'model' in locals():\n    del model\ntorch.cuda.empty_cache()\ngc.collect()\n\nmodel, _, preprocess = open_clip.create_model_and_transforms(MODEL, pretrained=PRETRAINED, device=device)\nmodel.eval()\n\n# Memory monitoring\nif torch.cuda.is_available():\n    print(f\"GPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f}GB allocated\")\n\nframes_root = 'data/video_frames'\nout_dir = 'data/clip_features'\nos.makedirs(out_dir, exist_ok=True)\n\n# Check if frames directory exists before proceeding\nif not os.path.exists(frames_root):\n    raise FileNotFoundError(f\"Frames directory not found: {frames_root}. Smart sampling may have failed.\")\n\ndef encode_batch(img_paths, target_size=256):\n    \"\"\"Encode batch with adaptive resolution based on content\"\"\"\n    ims = []\n    for p in img_paths:\n        im = Image.open(p).convert('RGB')\n        # Resize to target resolution (256x256 for balance of context vs memory)\n        im = im.resize((target_size, target_size), Image.LANCZOS)\n        ims.append(preprocess(im))\n    \n    with torch.no_grad():\n        batch = torch.stack(ims).to(device)\n        feats = model.encode_image(batch)\n        feats = feats / feats.norm(dim=-1, keepdim=True)\n        return feats.cpu().float().numpy()\n\n# Process videos with checkpointing\ncheckpoint_dir = 'data/checkpoints'\nos.makedirs(checkpoint_dir, exist_ok=True)\n\nfor vid in sorted(os.listdir(frames_root)):\n    vid_dir = os.path.join(frames_root, vid)\n    if not os.path.isdir(vid_dir):\n        continue\n    \n    # Check for existing checkpoint\n    checkpoint_file = os.path.join(checkpoint_dir, f'{vid}.checkpoint')\n    final_file = os.path.join(out_dir, f'{vid}.pkl')\n    \n    if os.path.exists(final_file):\n        print(f\"Skipping {vid}: already processed\")\n        continue\n    \n    imgs = [f for f in os.listdir(vid_dir) if f.lower().endswith('.jpg')]\n    if not imgs:\n        continue\n    imgs = sorted(imgs, key=lambda x: int(os.path.splitext(x)[0]))\n    file_paths = [f'./data/video_frames/{vid}/{name}' for name in imgs]\n    \n    feats_list = []\n    bs = 16  # Reduced batch size from 32 to save memory\n    \n    start_idx = 0\n    if os.path.exists(checkpoint_file):\n        # Load checkpoint\n        with open(checkpoint_file, 'rb') as f:\n            checkpoint = pickle.load(f)\n            feats_list = checkpoint['feats_list']\n            start_idx = checkpoint['last_idx']\n        print(f\"Resuming {vid} from frame {start_idx}\")\n    \n    for i in tqdm(range(start_idx, len(imgs), bs), desc=f'Encoding {vid}'):\n        batch_paths = [os.path.join(vid_dir, name) for name in imgs[i:i+bs]]\n        batch_feats = encode_batch(batch_paths)\n        feats_list.append(batch_feats)\n        \n        # Save checkpoint every 100 frames\n        if (i - start_idx) % 100 == 0 and i > start_idx:\n            with open(checkpoint_file, 'wb') as f:\n                pickle.dump({\n                    'feats_list': feats_list,\n                    'last_idx': i + bs\n                }, f)\n        \n        # Force garbage collection and memory cleanup\n        if i % 64 == 0:\n            gc.collect()\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n                if i % 128 == 0:\n                    print(f\"  GPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f}GB allocated\")\n    \n    # Finalize features\n    feats_np = np.concatenate(feats_list, axis=0)\n    with open(final_file, 'wb') as f:\n        pickle.dump((file_paths, feats_np), f)\n    \n    # Clean up checkpoint\n    if os.path.exists(checkpoint_file):\n        os.remove(checkpoint_file)\n    \n    print(f\"✓ Encoded {vid}: {feats_np.shape[0]} features at 256px resolution\")\n    \n# 4B.3 Build model and patch .env\nprint(\"Building NitzcheCLIP model...\")\nfrom nitzche_clip import NitzcheCLIP\nm = NitzcheCLIP(out_dir)\nos.makedirs('models', exist_ok=True)\nm.save('models/clip_siglip.pkl')\nenvp = Path('.env')\ncontent = envp.read_text(encoding='utf-8') if envp.exists() else ''\nlines = []\nsaw_path = saw_16 = False\nfor line in content.splitlines():\n    if line.strip().startswith('MODEL_PATH='): lines.append('MODEL_PATH=\"./models/\"'); saw_path=True\n    elif line.strip().startswith('MODEL_16='): lines.append('MODEL_16=\"clip_siglip.pkl\"'); saw_16=True\n    else: lines.append(line)\nif not saw_path: lines.append('MODEL_PATH=\"./models/\"')\nif not saw_16: lines.append('MODEL_16=\"clip_siglip.pkl\"')\n# Also set matching text encoder\nset_name = False; set_pre = False\nout=[]\nfor line in lines:\n    if line.strip().startswith('CLIP_MODEL_NAME='): out.append(f'CLIP_MODEL_NAME=\"{MODEL}\"'); set_name=True\n    elif line.strip().startswith('CLIP_PRETRAINED='): out.append(f'CLIP_PRETRAINED=\"{PRETRAINED}\"'); set_pre=True\n    else: out.append(line)\nif not set_name: out.append(f'CLIP_MODEL_NAME=\"{MODEL}\"')\nif not set_pre: out.append(f'CLIP_PRETRAINED=\"{PRETRAINED}\"')\nenvp.write_text('\\n'.join(out)+'\\n', encoding='utf-8')\nprint('✅ Recompute complete. Smart-sampled frames + SigLIP-256 features with intelligent adaptive sampling. .env updated.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "start_backend"
   },
   "outputs": [],
   "source": [
    "# Step 5) Start backend API (daemon)\n",
    "%cd /content/aic-25\n",
    "!python tools/aic_cli.py serve --port 8000 --run --daemon --no-reload\n",
    "!python tools/aic_cli.py serve-status\n",
    "import time, requests\n",
    "for _ in range(30):\n",
    "        \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "    try:\n",
    "        r = requests.get('http://localhost:8000/docs', timeout=2)\n",
    "        print('Backend reachable:', r.status_code)\n",
    "        break\n",
    "    except Exception:\n",
    "        time.sleep(1)\n",
    "else:\n",
    "    print('Backend not reachable')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_query"
   },
   "outputs": [],
   "source": [
    "# Step 6) Prepare a KIS query\n",
    "%cd /content/aic-25\n",
    "query_text = 'Cảnh quay bằng flycam một cây cầu ở TP Hồ Chí Minh, tiếp theo đến cảnh quay tòa nhà Bitexco. Một vài cảnh sau đó chuyển qua quay hình ảnh hồ gươm tại Hà Nội.'  # edit your KIS query here\n",
    "print('Query:', (query_text[:120] + ('...' if len(query_text) > 120 else '')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export_kis"
   },
   "outputs": [],
   "source": [
    "# Step 6) Export KIS CSV to submission/\n",
    "%cd /content/aic-25\n",
    "!python tools/aic_cli.py export --text \"Cảnh quay bằng flycam một cây cầu ở TP Hồ Chí Minh, tiếp theo đến cảnh quay tòa nhà Bitexco. Một vài cảnh sau đó chuyển qua quay hình ảnh hồ gươm tại Hà Nội.\" --task kis --name query-1 --api http://localhost:8000 --outdir submission --wait-api 30\n",
    "!echo 'Generated files:' && ls -la submission\n",
    "!echo 'Preview:' && head -n 5 submission/query-1-kis.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zip_submit"
   },
   "outputs": [],
   "source": [
    "# Step 7) Zip for Codabench\n",
    "%cd /content/aic-25\n",
    "!python tools/aic_cli.py zip-submission --outdir submission --name aic25_submission.zip\n",
    "from google.colab import files as colab_files\n",
    "colab_files.download('aic25_submission.zip')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}