{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# AIC-25 Colab: Textual KIS (Clean Flow)\n",
    "\n",
    "Choose one path and follow the numbered steps.\n",
    "\n",
    "- Path A — Quickstart (use provided features): simplest, fastest\n",
    "- Path B — Recompute (SigLIP2): higher quality, recomputes features and model\n",
    "\n",
    "Steps overview\n",
    "1) Clone repo\n",
    "2) Install deps\n",
    "3) Download dataset\n",
    "4A) Quickstart setup (Path A) — OR — 4B) Recompute SigLIP2 (Path B)\n",
    "5) Start backend\n",
    "6) Run a KIS query and export CSV\n",
    "7) Zip for Codabench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "choose_path"
   },
   "outputs": [],
   "source": [
    "# Choose your path: 'quickstart' or 'recompute'\n",
    "PATH_CHOICE = 'recompute'  # <-- set to 'quickstart' for the simplest path\n",
    "print('Path:', PATH_CHOICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# Step 1) Clone the repo into /content/aic-25\n",
    "import os, shutil, subprocess\n",
    "REPO_URL = 'https://github.com/dnlqvu/hcm-AI-challenge-2024-main.git'\n",
    "TARGET_DIR = '/content/aic-25'\n",
    "if os.path.exists(TARGET_DIR):\n",
    "    shutil.rmtree(TARGET_DIR)\n",
    "print('Cloning', REPO_URL, '->', TARGET_DIR)\n",
    "subprocess.run(['git', 'clone', REPO_URL, TARGET_DIR], check=True)\n",
    "print(os.listdir(TARGET_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cd_root"
   },
   "outputs": [],
   "source": [
    "%cd /content/aic-25\n",
    "!echo 'CWD:' && pwd && echo 'Top-level:' && ls -la | head -n 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Step 2) Install backend + extractor dependencies\n",
    "%cd /content/aic-25/aic-24-BE\n",
    "!python -m pip install --quiet --upgrade pip\n",
    "!pip install --quiet -r requirements.txt\n",
    "# Check if critical packages installed successfully\n",
    "import sys\n",
    "try:\n",
    "    import uvicorn\n",
    "    import fastapi\n",
    "    print(\"✓ Backend dependencies installed\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Backend dependency missing: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "%cd /content/aic-25\n",
    "# Extras for local extraction & utilities\n",
    "!pip install --quiet opencv-python tqdm pillow open_clip_torch\n",
    "# Verify extraction dependencies\n",
    "try:\n",
    "    import cv2\n",
    "    import open_clip\n",
    "    import torch\n",
    "    print(f\"✓ Extraction dependencies installed (torch device: {'cuda' if torch.cuda.is_available() else 'cpu'})\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Extraction dependency missing: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_csv"
   },
   "outputs": [],
   "source": [
    "# Step 3) Upload AIC_2025_dataset_download_link.csv (or set CSV_PATH)\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # choose AIC_2025_dataset_download_link.csv\n",
    "CSV_PATH = next(iter(uploaded))\n",
    "print('Using CSV:', CSV_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_dataset"
   },
   "outputs": [],
   "source": [
    "# Step 3) Download dataset assets to example_dataset/ and extract\n",
    "%cd /content/aic-25\n",
    "!python tools/aic_cli.py download-dataset --csv $CSV_PATH --outdir example_dataset --extract\n",
    "!ls -la example_dataset | head -n 50\n",
    "# Sanity checks\n",
    "!test -d example_dataset/map-keyframes || echo 'MISSING: example_dataset/map-keyframes'\n",
    "!test -d example_dataset/clip-features-32 || echo 'MISSING: example_dataset/clip-features-32'\n",
    "!test -d example_dataset/media-info || echo 'MISSING: example_dataset/media-info'\n",
    "!test -d example_dataset/keyframes || echo 'MISSING: example_dataset/keyframes'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pathA_quickstart"
   },
   "outputs": [],
   "source": [
    "%cd /content/aic-25\n",
    "if PATH_CHOICE != 'recompute':\n",
    "    print('Skipping Recompute (PATH_CHOICE!=recompute)')\n",
    "    raise SystemExit(0)\n",
    "VIDEOS_DIR = 'example_dataset/Videos_L21_a'\n",
    "MODEL = 'ViT-L-16-SigLIP-384'\n",
    "PRETRAINED = 'webli'  # Correct pretrained tag for SigLIP models\n",
    "CLIP_LEN = 1.5\n",
    "DECODE_FPS = 1.5  # Reduced from 2.0 to save memory\n",
    "TARGET_FPS = 0.8  # Reduced from 1.0 to save memory\n",
    "# 4B.1 Smart sampling → extract exact frames (original indices) - with memory optimization\n",
    "!python tools/aic_cli.py sample-smart --strategy clip-delta --videos-dir $VIDEOS_DIR \\\n",
    "    --frames-dir aic-24-BE/data/video_frames --decode-fps $DECODE_FPS --target-fps $TARGET_FPS \\\n",
    "    --model $MODEL --pretrained $PRETRAINED\n",
    "\n",
    "# 4B.2 Encode sampled frames with SigLIP2 and write shards\n",
    "%cd /content/aic-25/aic-24-BE\n",
    "import os, numpy as np, torch, pickle, gc\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "from tqdm import tqdm\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Loading {MODEL} with {PRETRAINED} on {device}...\")\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(MODEL, pretrained=PRETRAINED, device=device)\n",
    "model.eval()\n",
    "frames_root = 'data/video_frames'\n",
    "out_dir = 'data/clip_features'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Check if frames directory exists before proceeding\n",
    "if not os.path.exists(frames_root):\n",
    "    raise FileNotFoundError(f\"Frames directory not found: {frames_root}. Smart sampling may have failed.\")\n",
    "\n",
    "def encode_batch(img_paths):\n",
    "    ims=[]\n",
    "    for p in img_paths:\n",
    "        im = Image.open(p).convert('RGB')\n",
    "        ims.append(preprocess(im))\n",
    "    with torch.no_grad():\n",
    "        batch = torch.stack(ims).to(device)\n",
    "        feats = model.encode_image(batch)\n",
    "        feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "        return feats.cpu().float().numpy()\n",
    "\n",
    "for vid in sorted(os.listdir(frames_root)):\n",
    "    vid_dir = os.path.join(frames_root, vid)\n",
    "    if not os.path.isdir(vid_dir):\n",
    "        continue\n",
    "    imgs = [f for f in os.listdir(vid_dir) if f.lower().endswith('.jpg')]\n",
    "    if not imgs:\n",
    "        continue\n",
    "    imgs = sorted(imgs, key=lambda x: int(os.path.splitext(x)[0]))\n",
    "    file_paths = [f'./data/video_frames/{vid}/{name}' for name in imgs]\n",
    "    feats_list=[]\n",
    "    bs=32  # Reduced batch size from 64 to save memory\n",
    "    for i in tqdm(range(0, len(imgs), bs), desc=f'Encoding {vid}'):\n",
    "        batch_paths = [os.path.join(vid_dir, name) for name in imgs[i:i+bs]]\n",
    "        feats_list.append(encode_batch(batch_paths))\n",
    "        # Force garbage collection to free memory\n",
    "        if i % 128 == 0:\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    feats_np = np.concatenate(feats_list, axis=0)\n",
    "    with open(os.path.join(out_dir, f'{vid}.pkl'), 'wb') as f:\n",
    "        pickle.dump((file_paths, feats_np), f)\n",
    "    print(f\"Encoded {vid}: {feats_np.shape[0]} features\")\n",
    "    \n",
    "# 4B.3 Build model and patch .env\n",
    "print(\"Building NitzcheCLIP model...\")\n",
    "from nitzche_clip import NitzcheCLIP\n",
    "m = NitzcheCLIP(out_dir)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "m.save('models/clip_siglip.pkl')\n",
    "envp = Path('.env')\n",
    "content = envp.read_text(encoding='utf-8') if envp.exists() else ''\n",
    "lines = []\n",
    "saw_path = saw_16 = False\n",
    "for line in content.splitlines():\n",
    "    if line.strip().startswith('MODEL_PATH='): lines.append('MODEL_PATH=\"./models/\"'); saw_path=True\n",
    "    elif line.strip().startswith('MODEL_16='): lines.append('MODEL_16=\"clip_siglip.pkl\"'); saw_16=True\n",
    "    else: lines.append(line)\n",
    "if not saw_path: lines.append('MODEL_PATH=\"./models/\"')\n",
    "if not saw_16: lines.append('MODEL_16=\"clip_siglip.pkl\"')\n",
    "# Also set matching text encoder\n",
    "set_name = False; set_pre = False\n",
    "out=[]\n",
    "for line in lines:\n",
    "    if line.strip().startswith('CLIP_MODEL_NAME='): out.append(f'CLIP_MODEL_NAME=\"{MODEL}\"'); set_name=True\n",
    "    elif line.strip().startswith('CLIP_PRETRAINED='): out.append(f'CLIP_PRETRAINED=\"{PRETRAINED}\"'); set_pre=True\n",
    "    else: out.append(line)\n",
    "if not set_name: out.append(f'CLIP_MODEL_NAME=\"{MODEL}\"')\n",
    "if not set_pre: out.append(f'CLIP_PRETRAINED=\"{PRETRAINED}\"')\n",
    "envp.write_text('\\n'.join(out)+'\\n', encoding='utf-8')\n",
    "print('✅ Recompute complete. Smart-sampled frames + SigLIP2 features. .env updated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "start_backend"
   },
   "outputs": [],
   "source": [
    "# Step 5) Start backend API (daemon)\n",
    "%cd /content/aic-25\n",
    "!python tools/aic_cli.py serve --port 8000 --run --daemon --no-reload\n",
    "!python tools/aic_cli.py serve-status\n",
    "import time, requests\n",
    "for _ in range(30):\n",
    "        \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "    try:\n",
    "        r = requests.get('http://localhost:8000/docs', timeout=2)\n",
    "        print('Backend reachable:', r.status_code)\n",
    "        break\n",
    "    except Exception:\n",
    "        time.sleep(1)\n",
    "else:\n",
    "    print('Backend not reachable')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_query"
   },
   "outputs": [],
   "source": [
    "# Step 6) Prepare a KIS query\n",
    "%cd /content/aic-25\n",
    "query_text = 'Cảnh quay bằng flycam một cây cầu ở TP Hồ Chí Minh, tiếp theo đến cảnh quay tòa nhà Bitexco. Một vài cảnh sau đó chuyển qua quay hình ảnh hồ gươm tại Hà Nội.'  # edit your KIS query here\n",
    "print('Query:', (query_text[:120] + ('...' if len(query_text) > 120 else '')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export_kis"
   },
   "outputs": [],
   "source": [
    "# Step 6) Export KIS CSV to submission/\n",
    "%cd /content/aic-25\n",
    "!python tools/aic_cli.py export --text \"Cảnh quay bằng flycam một cây cầu ở TP Hồ Chí Minh, tiếp theo đến cảnh quay tòa nhà Bitexco. Một vài cảnh sau đó chuyển qua quay hình ảnh hồ gươm tại Hà Nội.\" --task kis --name query-1 --api http://localhost:8000 --outdir submission --wait-api 30\n",
    "!echo 'Generated files:' && ls -la submission\n",
    "!echo 'Preview:' && head -n 5 submission/query-1-kis.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zip_submit"
   },
   "outputs": [],
   "source": [
    "# Step 7) Zip for Codabench\n",
    "%cd /content/aic-25\n",
    "!python tools/aic_cli.py zip-submission --outdir submission --name aic25_submission.zip\n",
    "from google.colab import files as colab_files\n",
    "colab_files.download('aic25_submission.zip')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
