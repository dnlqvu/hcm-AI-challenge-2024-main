{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# AIC-25 Colab: Textual KIS (CLI flow)\n",
        "\n",
        "This notebook clones the repo, downloads dataset assets from a CSV of links, wires the dataset into the backend, starts the API, and exports KIS CSVs using the CLI tools.\n",
        "\n",
        "Note: TRAKE requires Sonic + transcripts/heading JSON ingestion and is not covered here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone_repo"
      },
      "outputs": [],
      "source": [
        "# Clone the repo into /content/aic-25\n",
        "import os, shutil, subprocess\n",
        "REPO_URL = \"https://github.com/dnlqvu/hcm-AI-challenge-2024-main.git\"\n",
        "TARGET_DIR = \"/content/aic-25\"\n",
        "\n",
        "if os.path.exists(TARGET_DIR):\n",
        "    shutil.rmtree(TARGET_DIR)\n",
        "print('Cloning', REPO_URL, '->', TARGET_DIR)\n",
        "subprocess.run(['git', 'clone', REPO_URL, TARGET_DIR], check=True)\n",
        "\n",
        "# List top-level files to confirm\n",
        "print(os.listdir(TARGET_DIR))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "# Install backend dependencies\n",
        "%cd /content/aic-25/aic-24-BE\n",
        "!python -m pip install --upgrade pip\n",
        "!pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upload_csv"
      },
      "outputs": [],
      "source": [
        "# Upload AIC_2025_dataset_download_link.csv (or edit CSV_PATH to an accessible path)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # choose AIC_2025_dataset_download_link.csv\n",
        "CSV_PATH = next(iter(uploaded))\n",
        "print('Using CSV:', CSV_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_dataset"
      },
      "outputs": [],
      "source": [
        "# Download dataset assets to example_dataset/ and extract\n",
        "%cd /content/aic-25\n",
        "!python tools/aic_cli.py download-dataset --csv $CSV_PATH --outdir example_dataset --extract\n",
        "!ls -la example_dataset | head -n 50\n",
        "# Sanity checks for required subfolders (maps, features, media-info, keyframes)\n",
        "!test -d example_dataset/map-keyframes || echo 'MISSING: example_dataset/map-keyframes (run download with --extract or upload and extract map-keyframes zip)'\n",
        "!test -d example_dataset/clip-features-32 || echo 'MISSING: example_dataset/clip-features-32 (run download with --extract or upload and extract features zip)'\n",
        "!test -d example_dataset/media-info || echo 'MISSING: example_dataset/media-info (run download with --extract or upload and extract media-info zip)'\n",
        "!test -d example_dataset/keyframes || echo 'MISSING: example_dataset/keyframes (run download with --extract or upload and extract keyframes zip)'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "recompute_siglip_intro"
      },
      "source": [
        "## Recommended: Recompute Features (SigLIP2)\n",
        "\n",
        "This path re-encodes frames with a stronger text-aligned model (SigLIP2) and rebuilds the backend model.\n",
        "- Uses Lighthouse decoding if available (`vendor/lighthouse-main`), otherwise falls back to OpenCV midpoints.\n",
        "- Skips the older precomputed features; you don't need to run `setup-example`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "recompute_siglip_install"
      },
      "outputs": [],
      "source": [
        "%cd /content/aic-25\n",
        "# Minimal deps for local extraction (open_clip is already in backend requirements)\n",
        "!python -m pip install --quiet --upgrade opencv-python tqdm pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "recompute_siglip_run"
      },
      "outputs": [],
      "source": [
        "%cd /content/aic-25\n",
        "VIDEOS_DIR = 'example_dataset/Videos_L21_a'\n",
        "MODEL = 'ViT-L-16-SigLIP-384'  # SigLIP2-like (adjust as desired)\n",
        "PRETRAINED = 'hf-hub:google/siglip-so400m-patch14-384'  # pretrained tag in open_clip\n",
        "CLIP_LEN = 1.5\n",
        "OUTDIR = 'hero_colab_out/clip-vit_features'\n",
        "# Prefer Lighthouse decoding when present\n",
        "import os\n",
        "BACKEND = 'lighthouse-clip' if os.path.isdir('vendor/lighthouse-main') else 'clip'\n",
        "print('Using backend:', BACKEND)\n",
        "\n",
        "# 1) Extract features\n",
        "!python tools/aic_cli.py clip-extract-colab --videos-dir $VIDEOS_DIR --outdir $OUTDIR --clip-len $CLIP_LEN --backend $BACKEND --model $MODEL --pretrained $PRETRAINED\n",
        "\n",
        "# 2) Convert features to shards + emit frame list\n",
        "!python tools/convert_hero_clip_to_shards.py --hero-clip-dir $OUTDIR --media-info aic-24-BE/data/media-info --clip-len $CLIP_LEN --outdir aic-24-BE/data/clip_features --emit-frame-list selected_frames_from_clip.csv\n",
        "\n",
        "# 3) Extract exact frames for those indices\n",
        "!python aic-24-BE/data_processing/crop_frame.py --input-dir $VIDEOS_DIR --output-dir aic-24-BE/data/video_frames --frame-list selected_frames_from_clip.csv\n",
        "\n",
        "# 4) Build model and patch .env with text encoder config\n",
        "!python tools/aic_cli.py build-model-from-shards --be-dir aic-24-BE --shards-dir aic-24-BE/data/clip_features --model-name clip_siglip.pkl\n",
        "\n",
        "# Patch .env to use matching text tower\n",
        "from pathlib import Path\n",
        "envp = Path('aic-24-BE/.env')\n",
        "content = envp.read_text(encoding='utf-8') if envp.exists() else ''\n",
        "lines = []\n",
        "saw_name = False; saw_pre = False\n",
        "for line in content.splitlines():\n",
        "    if line.strip().startswith('CLIP_MODEL_NAME='):\n",
        "        lines.append(f'CLIP_MODEL_NAME=\"{MODEL}\"'); saw_name=True\n",
        "    elif line.strip().startswith('CLIP_PRETRAINED='):\n",
        "        lines.append(f'CLIP_PRETRAINED=\"{PRETRAINED}\"'); saw_pre=True\n",
        "    else:\n",
        "        lines.append(line)\n",
        "if not saw_name: lines.append(f'CLIP_MODEL_NAME=\"{MODEL}\"')\n",
        "if not saw_pre: lines.append(f'CLIP_PRETRAINED=\"{PRETRAINED}\"')\n",
        "envp.write_text('\n'.join(lines)+'\n', encoding='utf-8')\n",
        "print('Recompute complete. .env updated with CLIP text model config.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smart_sampling_intro"
      },
      "source": [
        "## Optional: Smart Sampling (AI-driven)\n",
        "\n",
        "Use the smart sampler to select semantically novel frames (CLIP delta) or shot-aware frames, then extract only those.\n",
        "This replaces the provided keyframes; if you use this path, skip the next 'setup-example' cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smart_sampling_install"
      },
      "outputs": [],
      "source": [
        "%cd /content/aic-25\n",
        "# Install extras for smart sampling (opencv; transnetv2 optional)\n",
        "!python -m pip install --quiet --upgrade opencv-python\n",
        "# Optional (for shot-aware): !pip install --quiet transnetv2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smart_sampling_run"
      },
      "outputs": [],
      "source": [
        "%cd /content/aic-25\n",
        "VIDEOS_DIR = 'example_dataset/Videos_L21_a'  # set to your raw videos folder\n",
        "# Strategy options: 'clip-delta' (semantic change) or 'shots' (shot-aware)\n",
        "STRATEGY = 'clip-delta'\n",
        "DECODE_FPS = 2.0   # analysis decode rate for clip-delta\n",
        "TARGET_FPS = 1.0   # target kept frames/sec for clip-delta\n",
        "SHOT_DECODE_FPS = 10.0  # for shot-aware\n",
        "SHOT_LONG_SEC = 4.0     # long shot threshold (more samples)\n",
        "\n",
        "if STRATEGY == 'clip-delta':\n",
        "    !python tools/aic_cli.py sample-smart --strategy clip-delta --videos-dir $VIDEOS_DIR \\\n",
        "        --frames-dir aic-24-BE/data/video_frames --decode-fps $DECODE_FPS --target-fps $TARGET_FPS\n",
        "else:\n",
        "    !python tools/aic_cli.py sample-smart --strategy shots --videos-dir $VIDEOS_DIR \\\n",
        "        --frames-dir aic-24-BE/data/video_frames --shot-decode-fps $SHOT_DECODE_FPS --shot-long-sec $SHOT_LONG_SEC\n",
        "\n",
        "!ls -la aic-24-BE/data/video_frames | head -n 20\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smart_sampling_rebuild"
      },
      "outputs": [],
      "source": [
        "# Optional: Rebuild CLIP feature shards and model from smart-sampled frames (ViT-B/32)\n",
        "%cd /content/aic-25/aic-24-BE\n",
        "import os, numpy as np, torch, pickle\n",
        "from PIL import Image\n",
        "import open_clip\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k', device=device)\n",
        "model.eval()\n",
        "frames_root = 'data/video_frames'\n",
        "out_dir = 'data/clip_features'\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "def encode_batch(img_paths):\n",
        "    ims=[]\n",
        "    for p in img_paths:\n",
        "        im = Image.open(p).convert('RGB')\n",
        "        ims.append(preprocess(im))\n",
        "    with torch.no_grad():\n",
        "        batch = torch.stack(ims).to(device)\n",
        "        feats = model.encode_image(batch)\n",
        "        feats = feats / feats.norm(dim=-1, keepdim=True)\n",
        "        return feats.cpu().float().numpy()\n",
        "\n",
        "for vid in sorted(os.listdir(frames_root)):\n",
        "    vid_dir = os.path.join(frames_root, vid)\n",
        "    if not os.path.isdir(vid_dir):\n",
        "        continue\n",
        "    imgs = [f for f in os.listdir(vid_dir) if f.lower().endswith('.jpg')]\n",
        "    if not imgs:\n",
        "        continue\n",
        "    imgs = sorted(imgs, key=lambda x: int(os.path.splitext(x)[0]))\n",
        "    file_paths = [f'./data/video_frames/{vid}/{name}' for name in imgs]\n",
        "    feats_list=[]\n",
        "    bs=64\n",
        "    for i in tqdm(range(0, len(imgs), bs), desc=f'Encoding {vid}'):\n",
        "        batch_paths = [os.path.join(vid_dir, name) for name in imgs[i:i+bs]]\n",
        "        feats_list.append(encode_batch(batch_paths))\n",
        "    feats_np = np.concatenate(feats_list, axis=0)\n",
        "    with open(os.path.join(out_dir, f'{vid}.pkl'), 'wb') as f:\n",
        "        pickle.dump((file_paths, feats_np), f)\n",
        "\n",
        "# Build and save model pickle\n",
        "from nitzche_clip import NitzcheCLIP\n",
        "m = NitzcheCLIP(out_dir)\n",
        "os.makedirs('models', exist_ok=True)\n",
        "m.save('models/clip_vit_b32_nitzche.pkl')\n",
        "# Patch .env\n",
        "env_path = '.env'\n",
        "content = ''\n",
        "if os.path.exists(env_path):\n",
        "    content = open(env_path,'r',encoding='utf-8').read()\n",
        "lines = []\n",
        "seen_path = False; seen_16=False\n",
        "for line in content.splitlines():\n",
        "    if line.strip().startswith('MODEL_PATH='):\n",
        "        lines.append('MODEL_PATH=\"./models/\"')\n",
        "        seen_path=True\n",
        "    elif line.strip().startswith('MODEL_16='):\n",
        "        lines.append('MODEL_16=\"clip_vit_b32_nitzche.pkl\"')\n",
        "        seen_16=True\n",
        "    else:\n",
        "        lines.append(line)\n",
        "if not seen_path: lines.append('MODEL_PATH=\"./models/\"')\n",
        "if not seen_16: lines.append('MODEL_16=\"clip_vit_b32_nitzche.pkl\"')\n",
        "open(env_path,'w',encoding='utf-8').write('\n'.join(lines)+'\n')\n",
        "print('Model built and .env updated.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_dataset"
      },
      "outputs": [],
      "source": [
        "# Wire dataset into backend: copy, rename keyframes, pack features, build model\n",
        "%cd /content/aic-25\n",
        "# Guard: ensure required folders exist before setup (run the previous cell first)\n",
        "!test -d example_dataset/map-keyframes || (echo 'ERROR: Missing example_dataset/map-keyframes. Run the download cell first with --extract.' && false)\n",
        "!test -d example_dataset/clip-features-32 || (echo 'ERROR: Missing example_dataset/clip-features-32. Run the download cell first with --extract.' && false)\n",
        "!test -d example_dataset/media-info || (echo 'ERROR: Missing example_dataset/media-info. Run the download cell first with --extract.' && false)\n",
        "!test -d example_dataset/keyframes || (echo 'ERROR: Missing example_dataset/keyframes. Run the download cell first with --extract.' && false)\n",
        "!python tools/aic_cli.py setup-example --example-dir example_dataset\n",
        "!ls -la aic-24-BE/data/video_frames | head -n 20\n",
        "!ls -la aic-24-BE/data/clip_features | head -n 20\n",
        "!cat aic-24-BE/.env\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "start_backend"
      },
      "outputs": [],
      "source": [
        "# Start backend API in the background on port 8000\n",
        "%cd /content/aic-25\n",
        "# Use daemon mode so the notebook remains responsive\n",
        "!python tools/aic_cli.py serve --port 8000 --run --daemon --no-reload\n",
        "!python tools/aic_cli.py serve-status\n",
        "import time, requests\n",
        "for _ in range(30):\n",
        "    try:\n",
        "        r = requests.get('http://localhost:8000/docs', timeout=2)\n",
        "        print('Backend reachable:', r.status_code)\n",
        "        break\n",
        "    except Exception:\n",
        "        time.sleep(1)\n",
        "else:\n",
        "    print('Backend not reachable')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prepare_queries"
      },
      "outputs": [],
      "source": [
        "# Create a demo KIS query (inline; no files needed)\n",
        "%cd /content/aic-25\n",
        "query_text = 'tin t\u1ee9c th\u1eddi s\u1ef1'  # edit your KIS query here\n",
        "print('Query:', (query_text[:120] + ('...' if len(query_text) > 120 else '')))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "export_kis"
      },
      "outputs": [],
      "source": [
        "# Export KIS CSVs to submission/ (inline query)\n",
        "%cd /content/aic-25\n",
        "!python tools/aic_cli.py export --text \"$query_text\" --task kis --name query-1 --api http://localhost:8000 --outdir submission --wait-api 30\n",
        "!echo 'Generated files:' && ls -la submission\n",
        "!echo 'Preview:' && head -n 5 submission/query-1-kis.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "optional_zip"
      },
      "outputs": [],
      "source": [
        "# (Optional) Zip for Codabench and download\n",
        "%cd /content/aic-25\n",
        "!python tools/aic_cli.py zip-submission --outdir submission --name aic25_submission.zip\n",
        "from google.colab import files as colab_files\n",
        "colab_files.download('aic25_submission.zip')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}